%pip install xgboost
%pip install hyperopt
%pip install mlflow
%pip install findspark

import pandas as pd
import numpy as np
 
import pyspark.sql.functions as f
from pyspark.sql.types import *
from pyspark.sql import SparkSession

from pyspark.ml import PipelineModel
from pyspark.ml.feature import BucketedRandomProjectionLSHModel
 
from delta.tables import *
 
from xgboost import XGBClassifier
 
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score
from sklearn.utils.class_weight import compute_class_weight
 
from hyperopt import hp, fmin, tpe, SparkTrials, STATUS_OK, space_eval,Trials
 
import mlflow
import mlflow.pyfunc
from mlflow.tracking import MlflowClient
 
import time

spark = SparkSession.builder.appName('fuzzy_model').getOrCreate()
df = spark.read.option("header", "true").option("inferschema","true").csv("file:/Workspace/Shared/FuzzyOld_POC/FuzzyPanDataset.csv")

train_df = df.filter(df['apiProvider'] == 'PanKarza')
test_df = df.filter(df['apiProvider'] != 'PanKarza')

train_df = train_df.select([column for column in train_df.columns if column != 'Result'] + ['Result'])
test_df = test_df.select([column for column in test_df.columns if column != 'Result'] + ['Result'])

train_df = train_df.toPandas()
test_df = test_df.toPandas()

# X = train_df.drop(['Result'],axis=1).values
y = test_df['Result'].values
# X_train, X_other, y_train, y_other = train_test_split(X, y, train_size=0.70, stratify=y)
# Check the shape of the datasets
print("Train dataset shape:", train_df.shape)
print("Test dataset shape:", test_df.shape)

# Assuming 'Result' column is in both train_df and test_df
# Remove or modify these lines if necessary to reflect the actual column names
features = train_df.columns.drop('Result').tolist()

# Ensure that the datasets have the same number of samples
train_size = min(train_df.shape[0], test_df.shape[0])

# Split the datasets into training and test sets
X_train, X_other, y_train, y_other = train_test_split(train_df[features].values[:train_size], 
                                                     test_df['Result'].values[:train_size], 
                                                     train_size=0.70, 
                                                     stratify=y[:train_size])

# Verify the shapes of the splits
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_other shape:", X_other.shape)
print("y_other shape:", y_other.shape)


# define positive class scaling factor
weights = compute_class_weight(
  'balanced', 
  classes=np.unique(y_train), 
  y=y_train
  )
scale = weights[1]/weights[0]
 
# define hyperopt search space
search_space = {
    'max_depth' : hp.quniform('max_depth', 1, 30, 1)                                  
    ,'learning_rate' : hp.loguniform('learning_rate', np.log(0.01), np.log(0.40))     
    ,'gamma': hp.quniform('gamma', 0.0, 1.0, 0.001)                                   
    ,'min_child_weight' : hp.quniform('min_child_weight', 1, 20, 1)                   
    ,'subsample' : hp.loguniform('subsample', np.log(0.1), np.log(1.0))               
    ,'colsample_bytree' : hp.loguniform('colsample_bytree', np.log(0.1), np.log(1.0)) 
    ,'colsample_bylevel': hp.loguniform('colsample_bylevel', np.log(0.1), np.log(1.0))
    ,'colsample_bynode' : hp.loguniform('colsample_bynode', np.log(0.1), np.log(1.0)) 
    ,'scale_pos_weight' : hp.loguniform('scale_pos_weight', np.log(1), np.log(scale * 10))   
    }
 
# define function to clean up parameter values generated by hyperopt
def clean_params(hyperopt_params):
  
  # configure model parameters
  params = hyperopt_params
  
  if 'max_depth' in params: params['max_depth']=int(params['max_depth'])  
  if 'min_child_weight' in params: params['min_child_weight']=int(params['min_child_weight']) 
  if 'max_delta_step' in params: params['max_delta_step']=int(params['max_delta_step']) 
  
  # all other hyperparameters are taken as given by hyperopt
  
  # fixed parameters
  params['tree_method']='hist'
  params['predictor']='cpu_predictor'
  
  return params

def evaluate_model(hyperopt_params):
  
  # clean params  
  params = clean_params(hyperopt_params)
  
  # instantiate model with parameters
  model = XGBClassifier(**params)
  
  # train
  model.fit(X_train_broadcast.value, y_train_broadcast.value)
  
  # predict
  y_prob = model.predict_proba(X_validate_broadcast.value)
  
  # score
  model_ap = average_precision_score(y_validate_broadcast.value, y_prob[:,1])
  mlflow.log_metric('avg precision', model_ap)  # record actual metric with mlflow run
  
  # invert metric for hyperopt
  loss = -1 * model_ap  
  
  # return results
  return {'loss': loss, 'status': STATUS_OK}

# broadcast sets to workers for efficient model training
X_train_broadcast = sc.broadcast(X_train)
X_validate_broadcast = sc.broadcast(X_other)
 
y_train_broadcast = sc.broadcast(y_train)
y_validate_broadcast = sc.broadcast(y_other)